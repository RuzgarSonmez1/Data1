{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e97c0821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import regex as re\n",
    "\n",
    "from xmltodict import parse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2455b14",
   "metadata": {},
   "source": [
    "# HW3\n",
    "\n",
    "## Text Processing\n",
    "\n",
    "### Q1\n",
    "\n",
    "1. Modify the code I wrote in lecture 8 with what you have learnt in lecture 9 and correctly tokenize the text both on the word and sentence level, and by removing the stopwords. Rewrite the `getSummary` function and all the other functions that it depends by maing these corrections.\n",
    "\n",
    "2. Rewrite the code I wrote for `getKeywords` function making the same corrections.\n",
    "\n",
    "3. Test your code from parts 1 and 2 on random articles from the Guardian.\n",
    "\n",
    "4. Rewrite the `getSubjectGuardian` function for another newspaper in English, and test your code from part 1 and 2 on random articles from this new newspaper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9414750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubjectGuardian(subject):\n",
    "    with requests.get(f'https://www.theguardian.com/{subject}/rss') as link:\n",
    "        raw = parse(link.text)\n",
    "    return raw['rss']['channel']['item']\n",
    "nba = getSubjectGuardian('sport/nba')\n",
    "film = getSubjectGuardian('film')\n",
    "fashion = getSubjectGuardian('fashion')\n",
    "swEN = stopwords.words('english')\n",
    "\n",
    "def getText(url):\n",
    "    with requests.get(url) as link:\n",
    "        raw = BeautifulSoup(link.content,'html.parser')\n",
    "    return ' '.join([x.text for x in raw.find_all('p')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9afcdc",
   "metadata": {},
   "source": [
    "Q1 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84634fe9",
   "metadata": {},
   "source": [
    "Tokenize and clean sentences and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5179b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processText(text):\n",
    "    aa = {'sentences': sent_tokenize(text)}\n",
    "    aa.update({'cleanedSentences': [re.sub(r'[^\\p{Letter}\\s]','',sentence.lower()) for sentence in aa['sentences']]})\n",
    "    return [re.sub(r'[^\\w\\s]','',x.lower()) for x in aa['cleanedSentences']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aadf6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatrix(sentences):\n",
    "    swEN = stopwords.words('english')\n",
    "    vectorizer = CountVectorizer(stop_words=swEN)\n",
    "    return vectorizer.fit_transform(sentences)\n",
    "\n",
    "def getSummary(text,k):\n",
    "    sentences = processText(text)\n",
    "    matrix = getMatrix(sentences)\n",
    "    projection = PCA(n_components=1)\n",
    "    weights = projection.fit_transform(matrix.toarray())\n",
    "    res = list(zip(weights.transpose()[0],range(112),sentences))\n",
    "    tmp = sorted(res,key=lambda x: x[0],reverse=True)[:k]\n",
    "    return sorted(tmp, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5faf6d",
   "metadata": {},
   "source": [
    "Q1 - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c12be54",
   "metadata": {},
   "source": [
    "Tokenize and clean sentence and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ad20f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKeywords(text,sw,k):\n",
    "    aa = {'sentences': sent_tokenize(text)}\n",
    "    aa.update({'cleanedSentences': [re.sub(r'[^\\p{Letter}\\s]','',sentence.lower()) for sentence in aa['sentences']]})\n",
    "        \n",
    "    vectorizer = CountVectorizer(stop_words=sw)\n",
    "    matrix = vectorizer.fit_transform(aa['cleanedSentences'])\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    projection = PCA(n_components=1)\n",
    "    tmp = projection.fit_transform(matrix.transpose().toarray())\n",
    "    weights = tmp.transpose()[0]\n",
    "    \n",
    "    return sorted(zip(weights,words),key=lambda x: x[0], reverse=True)[:k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2510831",
   "metadata": {},
   "source": [
    "Q1 - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33373b63",
   "metadata": {},
   "source": [
    "Test functions from guardian newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "add050d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.9244743946362999, 23, 'this was a hollywood remake of the far superior spanish mystery thriller open your eyes see below in which a wealthy man gets cosmetic surgery to repair his face which has been ruined in a car crash and falls in love with a beautiful woman cruz'), (2.007925972298687, 51, 'noriega plays c√©sar a rich young guy who gets cosmetic surgery after being horribly disfigured in a car crash and then experiences ultrareal hallucinations indistinguishable from reality involving an affair with the fascinating young woman silvia cruz whose dalliance with him led to the crash'), (2.2771688961341443, 53, 'cruz won her best supporting actress oscar for this barcelonaset film from woody allen  a moderate comedy from allens luxurytourist euro period that also included his italian romp to rome with love  in which cruz played a stereotypically conceived italian call girl')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(5.578149859879137, 'cruz'),\n",
       " (1.3394502996794968, 'spanish'),\n",
       " (1.3187815036374662, 'role')]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = np.random.randint(0,4)\n",
    "text=getText(film[m]['link'])\n",
    "\n",
    "print(getSummary(text,3))\n",
    "getKeywords(text,swEN,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5e4512",
   "metadata": {},
   "source": [
    "Q1 -4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4e1bcc",
   "metadata": {},
   "source": [
    "Test functions with another newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96fccabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubjectNews(subject):\n",
    "    with requests.get(f'https://news.un.org/feed/subscribe/en/news/topic/{subject}/feed/rss.xml') as link:\n",
    "        raw = parse(link.text)\n",
    "    return raw['rss']['channel']['item']\n",
    "health = getSubjectNews('health')\n",
    "women = getSubjectNews('women')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fd817ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.5527553254319468, 0, 'subscribe audio hub womens rights are human rights and universal in times of war and peace a senior un official told the security council on wednesday urging ambassadors to ensure accountability for conflictrelated sexual violence'), (1.5882317075943726, 35, 'ms karkoutly cofounder of an organization for women lawyers called huquqyat outlined a list of actions for the council that included referring the situation in syria to the international criminal court adopting a resolution on detainees and missing persons investigating and prosecuting perpetrators of sexual violence and ensuring womens rights are at the heart of accountability efforts'), (5.57464828547666, 46, 'these countries were also asked to take a harder look at the prevailing view that supporting investigations of conflict elated sexual violence in ethiopia could somehow derail the proposed reform agenda of the current government\\xa0 widespread sexual violence against women and girls in conflict is being fueled by systemic impunity the united nations commission on human rights in south sudan said on monday')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(3.5223668913287107, 'violence'),\n",
       " (3.3184825556090654, 'sexual'),\n",
       " (1.6612743317623202, 'conflict')]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = np.random.randint(0,4)\n",
    "text=getText(women[m]['link'])\n",
    "\n",
    "print(getSummary(text,3))\n",
    "getKeywords(text,swEN,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c148a291",
   "metadata": {},
   "source": [
    "### Q2\n",
    "\n",
    "Write a function that returns all named entities (proper names, country names, corporation names only) from a URL. Function should take the URL as the input and must return the list of named entities from that URL. Test your code on random articles from the Guardian. Don't use the NLTK's NER that I demonstrated during the lecture. Use the SpaCY's NER function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "059a2fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gavin Millar', 'Alan Bennett', 'Dennis Potter', 'Victoria Wood', 'Dreamchild', 'Alice Liddell', 'Charles Dodgson', 'AKA', 'Lewis Carroll', 'Alice‚Äôs Adventures', 'Wonderland', 'Browne', 'Ian Holm', 'Dodgson', 'Millar', 'Alice', 'Alice', 'Jim Henson‚Äôs Creature Workshop', 'Millar', 'John Tenniel', 'Browne', 'Millar', 'Browne', 'Dreamchild', 'Andrew Sarris', 'the Village Voice', 'Peggy Ashcroft', 'Lionel Jeffries', 'the Prix Italia', 'Bennett‚Äôs Intensive Care', 'Talking Heads', 'Julie Walters', 'Victoria Wood:', 'Pat', 'Margaret', 'Wood', 'Housewife', 'Clydebank', 'Glasgow', 'Gavin', 'Rita', 'Osborne', 'Tom Millar', 'Gavin', 'Birmingham', 'King Edward‚Äôs', 'RAF', 'Christ Church', 'Oxford', 'Melvyn Bragg', 'Tempest', 'London', 'Sylvia Lane', 'Oxford', 'BBC', 'Karel Reisz‚Äôs', 'Listener', 'Arena Cinema', 'Woody Allen', 'Federico Fellini', 'Jean Renoir', 'Fran√ßois Truffaut', 'Monty Python‚Äôs Flying Circus', 'John Cleese', 'Gavin Millaaarrrrr', 'Millar', 'Isabel', 'Oxford', 'Scot', 'Roald Dahl‚Äôs', 'Jeremy Irons', 'Iain Banks‚Äôs', 'Millar', 'BBC', 'Banks‚Äôs The Crow Road', 'Evelyn Waugh‚Äôs', 'London Weekend Television', 'Harvey Fierstein', 'Fierstein', 'Stockard Channing', 'Denholm Elliott', 'John Le Carr√©‚Äôs', 'George Smiley', 'Kristin Scott Thomas', 'Belle √âpoque', 'The Ruth Rendell Mysteries', 'Foyle‚Äôs War', 'Albert Schweitzer', 'Jeroen Krabb√©', 'Sylvia', 'James, Tommy', 'Duncan', 'Kirstie', 'Isabel', 'Florence', 'Martha', 'Arwen', 'Gavin', 'Gavin Osborne Millar']\n"
     ]
    }
   ],
   "source": [
    "def func1(url):\n",
    "    with requests.get(url) as link:\n",
    "        raw = parse(link.text)\n",
    "        raw2= raw['rss']['channel']['item']\n",
    "    m = np.random.randint(0,4)\n",
    "    with requests.get(raw2[m]['link']) as link:\n",
    "        raw = BeautifulSoup(link.content,'html.parser')\n",
    "        last=' '.join([x.text for x in raw.find_all('p')])\n",
    "        NER = spacy.load(\"en_core_web_sm\")\n",
    "        res = NER(last)\n",
    "        print([i.text for i in res.ents if i.label_ in ('GPE','ORG','PERSON')])\n",
    "func1(f'https://www.theguardian.com/film/rss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d80aaa",
   "metadata": {},
   "source": [
    "### Q3\n",
    "\n",
    "1. Write a function that returns the most positive and the most negative sentences from a text. The function must take the text as the input and must return a 2-tuple: the first element as the most positive and the second as the most negative sentence with their polarity scores.\n",
    "\n",
    "2. Test your function on random articles from the Guardian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "587894b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Pandas(sentence='That‚Äôs questionable but, judging by the photo of the NFT that was posted on Instagram last month, they are at least too Cronenbergian to be destroyed.', polarityScore=-0.7096),\n",
       " Pandas(sentence='There you‚Äôll be, quite happy following them on social media when, bang, up pops a sub-Banksy cartoon of a monkey in a wig and a falsely jubilant message celebrating the fact that they‚Äôve joined some sort of club.', polarityScore=0.9134)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def polarity(text):\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    sentences = sent_tokenize(text)\n",
    "    score_list = []\n",
    "    \n",
    "    for temp, sent in enumerate(sentences):\n",
    "        score = analyser.polarity_scores(sent)\n",
    "        score_list.append({'sentence': sent, 'polarityScore': score['compound']})\n",
    "\n",
    "    df = pd.DataFrame(score_list).sort_values(by='polarityScore')\n",
    "    df1 = df.iloc[[0, -1]]\n",
    "    result = list(df1.itertuples(index=False))\n",
    "    return result\n",
    "m = np.random.randint(0,4)\n",
    "polarity(getText(film[m]['link']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
